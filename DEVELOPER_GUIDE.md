# Ad-Creative Insight Pipeline - Developer Guide

## ğŸ“‹ Overview

This developer guide provides a comprehensive overview of the Ad-Creative Insight Pipeline project structure, explaining the purpose of each file, module dependencies, and the data flow through the system.

## ğŸ—ï¸ Project Architecture

The pipeline follows a modular, step-by-step architecture with clean separation of concerns:

```
Raw Content â†’ Cleaning â†’ Structuring â†’ Deduplication â†’ Storage
```

Each step is isolated in its own module with clear interfaces, making the system maintainable and testable.

## ğŸ“ Project Structure

```
ad-creative-insight-pipeline/
â”œâ”€â”€ data_collection/          # Step 1: Data Collection
â”‚   â”œâ”€â”€ reddit_scraper.py
â”‚   â”œâ”€â”€ web_scraper.py
â”‚   â”œâ”€â”€ api_scraper.py
â”‚   â””â”€â”€ pipeline_entry.py
â”œâ”€â”€ cleaning/                 # Step 2: Text Cleaning
â”‚   â”œâ”€â”€ cleaner.py
â”‚   â””â”€â”€ cleaning_controller.py
â”œâ”€â”€ structuring/              # Step 3: LLM Structuring
â”‚   â”œâ”€â”€ insight_formatter.py
â”‚   â”œâ”€â”€ insight_schema.py
â”‚   â””â”€â”€ structuring_controller.py
â”œâ”€â”€ deduplication/            # Step 4: Duplicate Detection
â”‚   â”œâ”€â”€ similarity_checker.py
â”‚   â”œâ”€â”€ supabase_lookup.py
â”‚   â””â”€â”€ deduplication_controller.py
â”œâ”€â”€ supabase_storage/         # Step 5: Database Storage
â”‚   â”œâ”€â”€ supabase_client.py
â”‚   â”œâ”€â”€ insight_inserter.py
â”‚   â””â”€â”€ schema_reference.md
â”œâ”€â”€ utils/                    # Shared Utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ helpers.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ main.py                   # Pipeline Entry Point
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ .env.example             # Environment Variables Template
â””â”€â”€ README.md                # Project Documentation
```

## ğŸ”„ Data Flow & Module Hierarchy

### Pipeline Flow
1. **Data Collection** â†’ Raw text content (list of strings)
2. **Cleaning** â†’ Cleaned text content (list of strings)
3. **Structuring** â†’ Structured insights (list of dicts)
4. **Deduplication** â†’ Unique insights (filtered list of dicts)
5. **Storage** â†’ Stored insights with UUIDs

### Module Dependencies
```
main.py
â”œâ”€â”€ data_collection/pipeline_entry.py
â”‚   â”œâ”€â”€ data_collection/reddit_scraper.py
â”‚   â”œâ”€â”€ data_collection/web_scraper.py
â”‚   â””â”€â”€ data_collection/api_scraper.py
â”œâ”€â”€ cleaning/cleaning_controller.py
â”‚   â””â”€â”€ cleaning/cleaner.py
â”œâ”€â”€ structuring/structuring_controller.py
â”‚   â”œâ”€â”€ structuring/insight_formatter.py
â”‚   â””â”€â”€ structuring/insight_schema.py
â”œâ”€â”€ deduplication/deduplication_controller.py
â”‚   â”œâ”€â”€ deduplication/similarity_checker.py
â”‚   â””â”€â”€ deduplication/supabase_lookup.py
â”œâ”€â”€ supabase_storage/insight_inserter.py
â”‚   â””â”€â”€ supabase_storage/supabase_client.py
â””â”€â”€ utils/ (used across all modules)
    â”œâ”€â”€ config.py
    â”œâ”€â”€ helpers.py
    â””â”€â”€ logger.py
```

## ğŸ“„ File-by-File Breakdown

### ğŸ¯ Entry Point

#### `main.py`
**Purpose**: Main pipeline orchestrator that ties together all 5 steps
**Key Functions**:
- `run_pipeline(sources_config)` - Executes complete pipeline
- `main()` - Entry point with example configuration
**Dependencies**: All module controllers
**Data Flow**: Orchestrates data through all pipeline steps

---

### ğŸ” Step 1: Data Collection (`data_collection/`)

#### `pipeline_entry.py`
**Purpose**: Single entry point for all data collection activities
**Key Functions**:
- `collect_and_process_data(sources_config)` - Main collection orchestrator
**Dependencies**: All scraper modules
**Output**: List of raw text strings

#### `reddit_scraper.py`
**Purpose**: Scrape Reddit posts and comments for ad creative insights
**Key Functions**:
- `scrape_reddit_posts(subreddits, keywords, limit)` - Scrape posts from subreddits
- `scrape_reddit_comments(post_url, max_depth)` - Scrape comment threads
**Dependencies**: PRAW (Reddit API), utils/config
**Output**: Raw Reddit content

#### `web_scraper.py`
**Purpose**: Scrape blogs, websites, and Quora for content
**Key Functions**:
- `scrape_blog_posts(urls)` - Scrape blog content
- `scrape_website_content(url, selectors)` - Targeted website scraping
- `scrape_quora_answers(topic_urls)` - Quora answer extraction
**Dependencies**: BeautifulSoup, Selenium, requests
**Output**: Raw web content

#### `api_scraper.py`
**Purpose**: Pull content from social media APIs
**Key Functions**:
- `scrape_twitter_posts(hashtags, keywords, limit)` - Twitter/X API integration
- `scrape_meta_ads_library(search_terms, ad_type)` - Meta Ad Library
- `scrape_linkedin_posts(company_pages, keywords)` - LinkedIn content
**Dependencies**: Platform-specific API clients
**Output**: Raw social media content

---

### ğŸ§¹ Step 2: Cleaning (`cleaning/`)

#### `cleaning_controller.py`
**Purpose**: Main controller for text cleaning pipeline
**Key Functions**:
- `process_raw_content(raw_content_list)` - Process multiple raw texts
- `clean_single_text(raw_text)` - Clean individual text piece
**Dependencies**: cleaning/cleaner
**Input**: Raw text strings
**Output**: Cleaned text strings

#### `cleaner.py`
**Purpose**: Core text sanitization and normalization functions
**Key Functions**:
- `remove_html_tags(text)` - Strip HTML markup
- `remove_usernames_and_mentions(text)` - Remove @mentions and usernames
- `remove_urls(text)` - Strip URLs and links
- `remove_emojis_and_symbols(text)` - Clean emojis and special characters
- `truncate_to_paragraphs(text, max_paragraphs)` - Limit to 2-4 paragraphs
- `normalize_whitespace(text)` - Fix spacing and line breaks
**Dependencies**: re (regex), potentially NLTK
**Input**: Raw text
**Output**: Clean text

---

### ğŸ§  Step 3: Structuring (`structuring/`)

#### `structuring_controller.py`
**Purpose**: Main controller for converting clean text to structured insights
**Key Functions**:
- `process_cleaned_content(cleaned_content_list)` - Process multiple texts
- `structure_single_content(cleaned_text)` - Structure individual text
**Dependencies**: insight_formatter, insight_schema
**Input**: Cleaned text strings
**Output**: Validated structured insights (dicts)

#### `insight_formatter.py`
**Purpose**: LLM integration for text-to-structure conversion
**Key Functions**:
- `create_structuring_prompt(cleaned_text)` - Generate LLM prompt
- `call_llm_api(prompt)` - Make API call to LLM service
- `parse_llm_response(llm_response)` - Parse LLM JSON response
- `format_insight(cleaned_text)` - Complete formatting pipeline
**Dependencies**: OpenAI/Anthropic/Google APIs, utils/config
**Input**: Cleaned text
**Output**: Raw structured insight dict

#### `insight_schema.py`
**Purpose**: Define and validate insight data structure
**Key Functions**:
- `validate_insight_structure(insight_data)` - Validate required fields and types
- `sanitize_insight_data(insight_data)` - Clean and normalize data
**Schema**: INSIGHT, RESULTS, LIMITATIONS_CONTEXT, DIFFERENCE_SCORE (0-100)
**Input**: Raw insight dict
**Output**: Validated insight dict

---

### ğŸš« Step 4: Deduplication (`deduplication/`)

#### `deduplication_controller.py`
**Purpose**: Main controller for duplicate detection
**Key Functions**:
- `check_for_duplicates(new_insight)` - Check single insight for duplicates
- `batch_check_duplicates(new_insights)` - Process multiple insights
- `get_duplicate_statistics(new_insights)` - Generate duplicate stats
**Dependencies**: similarity_checker, supabase_lookup
**Input**: Structured insights
**Output**: Boolean (is_duplicate) or filtered unique insights

#### `similarity_checker.py`
**Purpose**: Calculate similarity between insights using various algorithms
**Key Functions**:
- `calculate_text_similarity(text1, text2)` - Text similarity scoring
- `calculate_insight_similarity(insight1, insight2)` - Overall insight similarity
- `is_duplicate_insight(new_insight, existing_insights, threshold)` - Duplicate detection
- `preprocess_for_comparison(insight)` - Normalize for comparison
**Dependencies**: sentence-transformers, scikit-learn, NLTK
**Algorithms**: Cosine similarity, semantic embeddings, fuzzy matching

#### `supabase_lookup.py`
**Purpose**: Query existing insights from database for comparison
**Key Functions**:
- `fetch_existing_insights(limit)` - Get all existing insights
- `fetch_insights_by_keywords(keywords, limit)` - Targeted keyword search
- `fetch_recent_insights(days, limit)` - Recent insights for comparison
- `get_relevant_insights_for_comparison(new_insight)` - Smart insight retrieval
**Dependencies**: supabase_storage/supabase_client
**Output**: List of existing insights for comparison

---

### ğŸ—ƒï¸ Step 5: Storage (`supabase_storage/`)

#### `supabase_client.py`
**Purpose**: Manage Supabase database connection and client
**Key Classes**:
- `SupabaseManager` - Connection management class
**Key Functions**:
- `initialize_supabase(url, key)` - Initialize client with credentials
- `get_supabase_client()` - Get global client instance
- `test_supabase_connection()` - Test database connectivity
**Dependencies**: supabase-py, utils/config
**Output**: Supabase client instance

#### `insight_inserter.py`
**Purpose**: Handle UUID generation and database insertion
**Key Functions**:
- `generate_insight_id()` - Create UUID for new insights
- `prepare_insight_for_storage(structured_insight)` - Format for database
- `insert_single_insight(client, insight)` - Insert one insight
- `batch_insert_insights(client, insights)` - Batch insert multiple insights
**Dependencies**: uuid, datetime, supabase_client
**Database Fields**: id, insight, results, limitations_context, difference_score, status='greylist', created_at, updated_at

#### `schema_reference.md`
**Purpose**: Complete documentation of Supabase database schema
**Contents**:
- Table structure and field definitions
- Indexes and constraints
- Sample SQL for table creation
- Status values and their meanings
**Use Case**: Reference for database setup and maintenance

---

### ğŸ§© Utilities (`utils/`)

#### `config.py`
**Purpose**: Centralized configuration management
**Key Classes**:
- `Config` - Main configuration class with all settings
**Key Functions**:
- `validate_required_settings()` - Check for missing config
- `get_api_config(service)` - Get service-specific API config
**Environment Variables**: API keys, thresholds, pipeline settings
**Dependencies**: python-dotenv

#### `helpers.py`
**Purpose**: General utility functions used across modules
**Key Functions**:
- `generate_uuid()` - UUID generation
- `get_current_timestamp()` - Timestamp utilities
- `retry_on_failure(max_retries, delay, backoff)` - Retry decorator
- `chunk_list(lst, chunk_size)` - List chunking for batch processing
- `measure_execution_time(func)` - Performance measurement decorator
**Use Cases**: Common operations, error handling, performance monitoring

#### `logger.py`
**Purpose**: Logging configuration and utilities
**Key Functions**:
- `setup_logger(name, level, log_file)` - Configure logger with console and file output
- `log_pipeline_step(logger, step_name, start_time, end_time)` - Log pipeline execution
**Features**: Console and file logging, configurable levels, timing information

---

### ğŸ“‹ Configuration Files

#### `requirements.txt`
**Purpose**: Python package dependencies
**Categories**:
- Core: supabase, python-dotenv
- Web scraping: requests, beautifulsoup4, selenium, scrapy
- APIs: praw, tweepy, facebook-sdk
- NLP: nltk, spacy, sentence-transformers
- LLM: openai, anthropic, google-generativeai
- Development: pytest, black, flake8

#### `.env.example`
**Purpose**: Template for environment variables
**Categories**:
- Database: Supabase URL and key
- APIs: Reddit, Twitter, Meta, LinkedIn credentials
- LLM: OpenAI, Anthropic, Google API keys
- Pipeline: Similarity thresholds, batch sizes, logging config

## ğŸ”„ Development Workflow

### 1. Setup
```bash
# Clone and setup environment
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your API keys
```

### 2. Implementation Order
1. **Utils first** - config, helpers, logger
2. **Storage setup** - supabase_client, database schema
3. **Individual modules** - implement each step independently
4. **Integration** - connect modules through controllers
5. **Main pipeline** - orchestrate complete flow

### 3. Testing Strategy
- **Unit tests** for individual functions
- **Integration tests** for module interactions
- **End-to-end tests** for complete pipeline
- **Mock external APIs** during development

### 4. Data Validation
- **Input validation** at each step
- **Schema validation** for structured data
- **Error handling** with graceful degradation
- **Logging** for debugging and monitoring

## ğŸ¯ Key Design Principles

1. **Modularity**: Each step is independent and testable
2. **Clean Interfaces**: Clear input/output contracts between modules
3. **Error Handling**: Graceful failure with detailed logging
4. **Configurability**: Environment-based configuration
5. **Scalability**: Batch processing and efficient algorithms
6. **Maintainability**: Clear documentation and consistent patterns

## ğŸš€ Getting Started

1. **Read this guide** to understand the architecture
2. **Set up your environment** with API keys and dependencies
3. **Start with utils** to establish common functionality
4. **Implement one module at a time** following the data flow
5. **Test each module** before moving to the next
6. **Integrate gradually** using the controller pattern
7. **Run the complete pipeline** with `python main.py`

This modular design ensures that you can develop, test, and maintain each component independently while maintaining a clean, scalable architecture for the entire pipeline.